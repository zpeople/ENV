{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PD Setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示所有列\n",
    "pd.set_option('display.max_columns', None)\n",
    "# 显示所有行\n",
    "pd.set_option('display.max_rows', None)\n",
    "#显示宽度\n",
    "pd.set_option('display.width', 2000)\n",
    "\n",
    "pd.set_option('display.unicode.ambiguous_as_wide', True)\n",
    "pd.set_option('display.unicode.east_asian_width', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "train_path =\"./data/train.csv\"\n",
    "test_path=\"./data/test.csv\"\n",
    "train_data = pd.read_csv(train_path)\n",
    "test_data =pd.read_csv(test_path)\n",
    "print('实验数据大小:',train_data.shape)\n",
    "print('预测数据大小:',test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "info（）\n",
    "\n",
    "* 行数（非空值计数）：显示每一列中有多少个非空值，这可以帮助你了解数据的完整性和是否存在缺失值。\n",
    "* 列名：展示 DataFrame 中所有列的名字。\n",
    "* 列类型（Dtype）：每列的数据类型（如 int64, float64, object 等），数据类型对于数据分析非常重要，因为不同类型的数据处理方式不同。\n",
    "* 内存使用情况：显示该 DataFrame 在内存中的占用大小，这对于优化性能和资源管理特别有用，特别是当你在处理非常大的数据集时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "describe()\n",
    "\n",
    "\n",
    "* count（计数）：非空值的数量。\n",
    "* mean（平均数）：每列数值的平均值。\n",
    "* std（标准差）：衡量数值的变异程度，即数值与平均值之间的偏离度。\n",
    "* min（最小值）：每列中的最小数值。\n",
    "* 25%（第一四分位数）：也称为下四分位数，表示有25%的数据小于等于这个数值。\n",
    "* 50%（第二四分位数/中位数）：数据中间的值，即一半的数据小于该值，另一半大于该值。\n",
    "* 75%（第三四分位数）：表示有75%的数据小于等于这个数值。\n",
    "* max（最大值）：每列中的最大数值。\n",
    "\n",
    "如果你的数据框中有非数值类型（例如对象或类别类型），你可能需要通过传递参数 include=['object', 'category'] 来查看这些类型列的描述性统计信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "value_counts（）\n",
    "\n",
    "用于计算分类变量中每个类别的出现频率（占比）的方法\n",
    "\n",
    "发现数据集严重不平衡，可能需要考虑采用重采样技术或其他策略来平衡数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "head()\n",
    "\n",
    "用于显示 DataFrame 的前几行数据的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "isnull().sum()\n",
    "\n",
    "用来表示数据中的缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = train_data.isna().sum()\n",
    "# 或者\n",
    "missing_values = train_data.isnull().sum()\n",
    "\n",
    "# 查看缺失值\n",
    "missing_values[missing_values > 0]\n",
    "\n",
    "\n",
    "# 查看某列包含缺失值（NaN）的前几条记录\n",
    "train_data[train_data['column_name'].isnull()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".nunique()\n",
    "\n",
    "用于返回对象中除 NA/null 值外的不同(unique)的非重复值的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".unique()\n",
    "\n",
    "查看某列里，所有不同的（唯一）值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = 'target'\n",
    "train_data[col_name].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".corr()\n",
    "\n",
    "* 'pearson'：标准的皮尔逊相关系数。\n",
    "* 'kendall'：Kendall Tau 相关系数。\n",
    "* 'spearman'：Spearman 秩相关系数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CorrDf = pd.DataFrame()\n",
    "CorrDf = train_data.corr(method='pearson', min_periods=1)\n",
    "\n",
    "#只显示与 target 相关性较高的特征，从而使得图表更加简洁和易于理解\n",
    "print(CorrDf['target'].sort_values())\n",
    "high_corr_features = CorrDf['target'].abs() > 0.03 \n",
    "corr_matrix_filtered = CorrDf.loc[high_corr_features, high_corr_features]\n",
    "\n",
    "plt.figure(figsize=(10, 8))  \n",
    "sns.heatmap(corr_matrix_filtered, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "\n",
    "# 显示图表\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* 数据清洗（Data Cleaning）：\n",
    "  \n",
    "处理缺失值：可以通过删除含有缺失值的记录、填充缺失值（如使用均值、中位数或众数填充）等方式进行。\n",
    "\n",
    "去除重复值：识别并删除数据集中的重复记录。\n",
    "\n",
    "纠正错误值：检查数据中的逻辑错误或异常值，并进行相应的修正。\n",
    "\n",
    "* 数据转换（Data Transformation）：\n",
    "  \n",
    "标准化/归一化：将数值属性缩放到某个特定范围，比如0-1之间，或者将其标准化以拥有零均值和单位方差。\n",
    "\n",
    "编码分类变量：如你之前提到的，可以使用LabelEncoder或OneHotEncoder对分类数据进行编码。\n",
    "\n",
    "特征构造：基于现有特征创建新的有意义的特征。\n",
    "\n",
    "* 数据集成（Data Integration）：\n",
    "  \n",
    "合并来自不同源的数据，确保一致性，解决实体识别问题，以及处理冗余和冲突的数据。\n",
    "\n",
    "* 数据规约（Data Reduction）：\n",
    "  \n",
    "降维：通过主成分分析（PCA）、线性判别分析（LDA）等技术减少数据维度。\n",
    "\n",
    "数量规约：使用参数模型或非参数模型来代替原始数据。\n",
    "\n",
    "* 数据离散化（Data Discretization）：\n",
    "  \n",
    "将连续型数据转换为离散区间，便于某些算法处理。\n",
    "\n",
    "* 数据分割（Data Splitting）：\n",
    "  \n",
    "将数据集划分为训练集、验证集和测试集，用于模型训练、调参和评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整合数据，可以一起处理\n",
    "full_data = pd.concat([train_data,test_data],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "\n",
    "#用一个特定的常数来填充所有的缺失值\n",
    "full_data.fillna(0, inplace=True) \n",
    "\n",
    "# 用特定字母\n",
    "full_data['column_name'].fillna('U')\n",
    "\n",
    "#使用上一个非缺失值来填充当前的缺失值。这种方法通常适用于时间序列数据\n",
    "full_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "#与前向填充相反，使用下一个非缺失值来填充当前的缺失值\n",
    "full_data.fillna(method='bfill', inplace=True)\n",
    "\n",
    "#均值/中位数/众数填充\n",
    "# 使用均值填充\n",
    "full_data['column_name'].fillna(full_data['column_name'].mean(), inplace=True)\n",
    "\n",
    "# 使用中位数填充\n",
    "full_data['column_name'].fillna(full_data['column_name'].median(), inplace=True)\n",
    "\n",
    "# 使用众数填充\n",
    "full_data['column_name'].fillna(full_data['column_name'].mode()[0], inplace=True)\n",
    "\n",
    "#插值法（Interpolation） 特别适合于时间序列数据，可以根据已有的数据点估计缺失值\n",
    "full_data.interpolate(method='linear', inplace=True)\n",
    "\n",
    "#出现频率最高的值来填充所有的缺失值\n",
    "full_data = full_data.fillna({'column_name': full_data['column_name'].value_counts().idxmax()})\n",
    "\n",
    "#从 Cabin 列中提取每个客舱的第一个字符来表示甲板（Deck）信息，并创建一个新的列 Deck 来存储这些信息。\n",
    "full_data['Deck'] = full_data['Cabin'].apply(lambda x: x[0] if x != 'Unknown' else 'Unknown')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Drop\n",
    "\n",
    "# 删除所有 column_name 列中值为 'some_value' 的行。\n",
    "full_data.drop(full_data[full_data['column_name'] == 'some_value'].index, inplace=True)\n",
    "\n",
    "# 删除含有任何缺失值的行\n",
    "full_data.dropna(inplace=True)\n",
    "\n",
    "# 删除含有任何缺失值的列\n",
    "full_data.dropna(axis=1, inplace=True)\n",
    "\n",
    "# 删除指定列\n",
    "full_data.drop('column_name', axis=1, inplace=True)\n",
    "full_data.drop(['column_name1', 'column_name2'], axis=1, inplace=True)\n",
    "\n",
    "#基于条件删除列\n",
    "thresh = len(full_data) * 0.6  # 至少需要60%的非空值\n",
    "full_data.dropna(axis=1, thresh=thresh, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# 避免污染到原始数据\n",
    "full_data =train_data.copy() \n",
    "# 对每个分类特征应用标签编码\n",
    "for column in full_data.columns:\n",
    "    if(full_data[column].dtype =='object'):\n",
    "        full_data[column] = le.fit_transform(full_data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Discretization\n",
    "\n",
    "# 对数变换 有助于减少数据中的偏斜(skewness)，特别是当存在大量小值和少量极大值时 使得数据分布更加接近正态分布\n",
    "full_data['column_name'] = np.log1p(full_data['column_name'])\n",
    "\n",
    "# 找出数值型特征（列），并计算这些特征的偏度（skewness）。然后，它会筛选出那些偏度绝对值大于1的特征，并准备对这些特征进行对数变换以减少数据偏斜\n",
    "numeric_df = full_data.select_dtypes(['float64','int32','int64'])\n",
    "numeric_cols = numeric_df.columns.tolist()\n",
    "skewed_cols = full_data[numeric_cols].apply(lambda x: x.skew()).sort_values(ascending=False)\n",
    "skewed_df = pd.DataFrame({'skew':skewed_cols})\n",
    "skew_cols = skewed_df[skewed_df['skew'].abs()>1].index.tolist()\n",
    "for col in skew_cols:\n",
    "    # 避免对含有0或负数的列直接取对数，可以先加上一个最小正值\n",
    "    min_val = full_data[col].min()\n",
    "    if min_val <= 0:\n",
    "        full_data[col] = full_data[col] - min_val + 1e-8  # 调整值域使所有值都大于0\n",
    "    full_data[col] = np.log(full_data[col])\n",
    "\n",
    "\n",
    "# 分箱（Binning）是一种将连续数据离散化的方法，可以减少异常值的影响，使模型更加稳定  \n",
    "# 划分到5个等宽的区间（bins）\n",
    "full_data['column_name'] = pd.cut(full_data['column_name'], bins=5, labels=False)\n",
    "\n",
    "\n",
    "#按自己的需求划分区间\n",
    "full_data.loc[full_data['column_name'] <= 7.91, 'column_name'] = 0\n",
    "full_data.loc[(full_data['column_name'] > 7.91) & (full_data['column_name'] <= 14.454), 'column_name'] = 1\n",
    "full_data.loc[(full_data['column_name'] > 14.454) & (full_data['column_name'] <= 31), 'column_name'] = 2\n",
    "full_data.loc[full_data['column_name'] > 31, 'column_name'] = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# retrieve english stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Converts text to lower case\n",
    "def convert_to_lowercase(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    if isinstance(text, str):\n",
    "        return text.lower()\n",
    "    return text\n",
    "\n",
    "# Remove all punctuation from the text\n",
    "def remove_punctuation(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    text = re.sub(f'[{string.punctuation}]', '', text)\n",
    "    return text\n",
    "\n",
    "# Removes all numbers from the text\n",
    "def remove_numbers(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "# Text segmentation, then remove the length of 2 or less and the single word and stop word\n",
    "def remove_short_words_and_stop_words(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if len(word) > 2 and word not in stop_words]\n",
    "    cleaned_text = ' '.join(words)\n",
    "    return cleaned_text\n",
    "\n",
    "# Replace two or more consecutive Spaces with a single space\n",
    "def remove_multiple_spaces(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    cleaned_text = re.sub(r' {2,}', ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Remove urls\n",
    "def remove_urls(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    url_pattern = r'(www.|http[s]?://)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    return re.sub(url_pattern, '', text)\n",
    "\n",
    "# Remove hmtmls\n",
    "def remove_html(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    html_entities = r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});'\n",
    "    return re.sub(html_entities, '', text)\n",
    "\n",
    "# Remove @ and #\n",
    "def remove_tags(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    tag_pattern = r'@([a-z0-9]+)|#'\n",
    "    return re.sub(tag_pattern, '', text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\d\\s]+', '',text)\n",
    "    word_list = []\n",
    "    for each_word in cleaned_text.split(' '):\n",
    "        word_list.append((each_word).lower())\n",
    "    word_list = [\n",
    "        WordNetLemmatizer().lemmatize(each_word.strip()) for each_word in word_list\n",
    "        if each_word not in stop_words and each_word.strip() != ''\n",
    "    ]\n",
    "    return \" \".join(word_list)\n",
    "\n",
    "\n",
    "\n",
    "def clear_text(df, col):\n",
    "    df[col] = df[col].apply(convert_to_lowercase)\n",
    "    df[col] = df[col].apply(remove_urls)\n",
    "    df[col] = df[col].apply(remove_html)\n",
    "    df[col] = df[col].apply(remove_tags)\n",
    "    df[col] = df[col].apply(remove_numbers)\n",
    "    df[col] = df[col].apply(remove_short_words_and_stop_words)\n",
    "    df[col] = df[col].apply(preprocess_text)\n",
    "    df[col] = df[col].apply(remove_multiple_spaces) \n",
    "    # df[col] = df[col].apply(remove_emoji) \n",
    "    # df[col] = df[col].apply(correct_spellings)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常见的回归算法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scoring\n",
    "\n",
    "* 分类任务：\n",
    "'accuracy': 准确率，正确预测的比例。\n",
    "'balanced_accuracy': 平衡准确率，适用于不平衡数据集。\n",
    "'precision', 'precision_macro', 'precision_micro', 'precision_weighted', 'precision_samples': 精确率，不同参数对应不同的平均策略。\n",
    "'recall', 'recall_macro', 'recall_micro', 'recall_weighted', 'recall_samples': 召回率，同样有不同的平均策略。\n",
    "'f1', 'f1_macro', 'f1_micro', 'f1_weighted', 'f1_samples': F1分数，是精确率和召回率的调和平均数。\n",
    "'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo': ROC曲线下面积，用于二分类或多分类。\n",
    "* 回归任务：\n",
    "'neg_mean_squared_error'(NMSE): 负均方误差，值越小越好。\n",
    "'neg_mean_absolute_error'(NMAE): 负平均绝对误差。\n",
    "'neg_mean_squared_log_error': 负均方对数误差。\n",
    "'neg_median_absolute_error': 负中位数绝对误差。\n",
    "'r2': R² (决定系数)，表示模型解释变异性的比例，值越大越好。\n",
    "'explained_variance': 解释方差得分。\n",
    "* 聚类任务：\n",
    "'adjusted_rand_score': 调整后的兰德指数，用于比较两个聚类结果的一致性。\n",
    "* 其他：\n",
    "'neg_log_loss': 负对数损失，适用于概率预测。\n",
    "'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_weighted', 'jaccard_samples': 杰卡德相似系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%skip\n",
    "\n",
    "#导入机器学习算法库\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, KFold\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "# 设置 kfold，交叉采样法拆分数据集\n",
    "kfold = KFold(n_splits=10)\n",
    "lgb = LGBMRegressor(objective='regression', force_col_wise=True, random_state=42)\n",
    "# xgb = XGBRegressor(objective='reg:linear', random_state=50, silent=True)\n",
    "# 汇总不同模型算法\n",
    "regressors = []\n",
    "regressors.append(LinearRegression())\n",
    "regressors.append(DecisionTreeRegressor())\n",
    "regressors.append(RandomForestRegressor())\n",
    "regressors.append(ExtraTreesRegressor())\n",
    "regressors.append(GradientBoostingRegressor())\n",
    "regressors.append(KNeighborsRegressor())\n",
    "regressors.append(Ridge())\n",
    "regressors.append(Lasso())\n",
    "regressors.append(SVR())\n",
    "# regressors.append(lgb)\n",
    "# regressors.append(xgb)\n",
    "\n",
    "cv_results=[]\n",
    "\n",
    "# 示例代码：使用交叉验证评估每个模型\n",
    "for regressor in regressors:\n",
    "    results = cross_val_score(regressor, X_train, Y_train, cv=kfold, scoring='neg_mean_squared_error')\n",
    "    cv_results.append(results)\n",
    "    # print(f\"{regressor.__class__.__name__}: Mean Squared Error: {-cv_results.mean()} (+/- {cv_results.std()})\") \n",
    "\n",
    "cv_means=[]\n",
    "cv_std=[]\n",
    "for cv_result in cv_results:\n",
    "    cv_means.append(cv_result.mean())\n",
    "    cv_std.append(cv_result.std())\n",
    "    \n",
    "    \n",
    "#汇总数据\n",
    "cvResDf=pd.DataFrame({'cv_mean':cv_means,\n",
    "                     'cv_std':cv_std,\n",
    "                     'algorithm':['LR','DT','RandomForestRe','ExtraTreesRe',\n",
    "                                  'GradientBoostingRe','KNN RE','Ridge','Lasso','SVR','lgb','xgb']})\n",
    "#neg_mean_squared_error 数值越接近 0，表示模型效果越好\n",
    "print(cvResDf)\n",
    "''' \n",
    "    cv_mean    cv_std           algorithm\n",
    "0 -0.018805  0.008046                  LR\n",
    "1 -0.044421  0.012643                  DT\n",
    "2 -0.019065  0.004311      RandomForestRe\n",
    "3 -0.017231  0.005352        ExtraTreesRe\n",
    "4 -0.015835  0.003744  GradientBoostingRe\n",
    "5 -0.064388  0.010701              KNN RE\n",
    "6 -0.018720  0.007841               Ridge\n",
    "7 -0.063309  0.011553               Lasso\n",
    "8 -0.052282  0.009065                 SVR\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找到某一个合适的Model，超参数调优\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "X_train =np.array(X_train)\n",
    "X_test =np.array(X_test)\n",
    "print(\"是否存在任何 NaN:\", np.isnan(X_train).any())\n",
    "parameters = {'n_estimators': [10, 50, 100, 200, 300, 400, 500],\n",
    "              'criterion': ['poisson', 'absolute_error', 'friedman_mse', 'squared_error'],\n",
    "              'max_features': ['sqrt', 'log2']}\n",
    "#n_estimators 模型中基础估计器（通常是决策树）的数量。\n",
    "model =RandomForestRegressor()\n",
    "grid_search = GridSearchCV(model, parameters, scoring='neg_mean_squared_error', n_jobs=4)\n",
    "\n",
    "grid_search.fit(X_train, Y_train)\n",
    "print('best parameters:',grid_search.best_params_)\n",
    "print('best score:',grid_search.best_score_)\n",
    "# 预测\n",
    "y_pred_rf = grid_search.predict(X_test)\n",
    "\n",
    "\n",
    "def save_data(y_pred,filename):\n",
    "    submission=pd.DataFrame ( {\n",
    "    \"Id\" : test_data [ \"Id\" ],\n",
    "    \"SalePrice\" : y_pred.astype (int)\n",
    "    })\n",
    "    submission.to_csv ( filename+'_result.csv',index=False)\n",
    "    \n",
    "save_data(y_pred_rf,'random')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常见的分类算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入机器学习算法库\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,ExtraTreesClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score,StratifiedKFold\n",
    "\n",
    "#设置kfold，交叉采样法拆分数据集\n",
    "kfold=StratifiedKFold(n_splits=5)\n",
    "\n",
    "#汇总不同模型算法\n",
    "classifiers=[]\n",
    "classifiers.append(SVC())\n",
    "classifiers.append(DecisionTreeClassifier())\n",
    "classifiers.append(RandomForestClassifier())\n",
    "classifiers.append(ExtraTreesClassifier())\n",
    "classifiers.append(GradientBoostingClassifier())\n",
    "classifiers.append(KNeighborsClassifier())\n",
    "classifiers.append(LogisticRegression())\n",
    "classifiers.append(LinearDiscriminantAnalysis())\n",
    "\n",
    "cv_results=[]\n",
    "for classifier in classifiers:\n",
    "    cv_results.append(cross_val_score(classifier,X_train,Y_train,\n",
    "                                      scoring='accuracy',cv=kfold,n_jobs=-1))\n",
    "    \n",
    "cv_means=[]\n",
    "cv_std=[]\n",
    "for cv_result in cv_results:\n",
    "    cv_means.append(cv_result.mean())\n",
    "    cv_std.append(cv_result.std())\n",
    "    \n",
    "#汇总数据\n",
    "cvResDf=pd.DataFrame({'cv_mean':cv_means,\n",
    "                     'cv_std':cv_std,\n",
    "                     'algorithm':['SVC','DecisionTreeCla','RandomForestCla','ExtraTreesCla',\n",
    "                                  'GradientBoostingCla','KNN','LR','LinearDiscrimiAna']})\n",
    "\n",
    "print(cvResDf)\n",
    "''' \n",
    "  cv_mean    cv_std            algorithm\n",
    "0  0.640864  0.011413                  SVC\n",
    "1  0.789053  0.028307      DecisionTreeCla\n",
    "2  0.781181  0.017948      RandomForestCla\n",
    "3  0.767723  0.021185        ExtraTreesCla\n",
    "4  0.826044  0.013216  GradientBoostingCla\n",
    "5  0.795725  0.019425                  KNN\n",
    "6  0.802473  0.008145                   LR\n",
    "7  0.794614  0.016101    LinearDiscrimiAna\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"是否存在任何 NaN:\", np.isnan(X_train).any())\n",
    "parameters ={'n_estimators':[10,50,100,200,300,400,500],'criterion':['log_loss','gini','entropy'],'max_features':['sqrt','log2']}\n",
    "#n_estimators 模型中基础估计器（通常是决策树）的数量。\n",
    "model =RandomForestClassifier()\n",
    "grid_search =GridSearchCV(model,parameters,scoring='accuracy',n_jobs=4) #n_jobs 用于指定并行作业的数量\n",
    "\n",
    "grid_search.fit(X_train, Y_train)\n",
    "print('best parameters:',grid_search.best_params_)\n",
    "print('best score:',grid_search.best_score_)\n",
    "# 预测\n",
    "y_pred_rf = grid_search.predict(X_test)\n",
    "\n",
    "save_data(y_pred_rf,'random')\n",
    "# 评估\n",
    "# best score: 0.8204506936162   --- 0.75119"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the evaluation metrics that we will use to assess our models\n",
    "\n",
    "def accuracy(predicted_labels, true_labels):\n",
    "    \"\"\"\n",
    "    Accuracy is correct predictions / all predicitons\n",
    "    \n",
    "    Args:\n",
    "        predicted_labels (np.ndarray[int, 1]): the integer labels from the predictions. Uni-dimensional\n",
    "        true_labels (np.ndarray[int, 1]): the integer labels from the gold standard. Uni-dimensional\n",
    "    \n",
    "    Returns:\n",
    "        accuracy_value (double)\n",
    "        \n",
    "    \"\"\"\n",
    "    accuracy_value = 0.\n",
    "    accuracy_value = predicted_labels[predicted_labels == true_labels].shape[0] / predicted_labels.shape[0]\n",
    "    return accuracy_value\n",
    "\n",
    "def precision(predicted_labels, true_labels):\n",
    "    \"\"\"\n",
    "    Precision is True Positives / All Positives Predictions\n",
    "    \n",
    "    Args:\n",
    "        predicted_labels (np.ndarray[int, 1]): the integer labels from the predictions. Uni-dimensional\n",
    "        true_labels (np.ndarray[int, 1]): the integer labels from the gold standard. Uni-dimensional\n",
    "    \n",
    "    Returns:\n",
    "        precision_value (double)\n",
    "        \n",
    "    \"\"\"\n",
    "    precision_value = 0.\n",
    "    TP = np.sum((predicted_labels == 1) & (true_labels == 1))\n",
    "    FP = np.sum((predicted_labels == 1) & (true_labels == 0))\n",
    "    precision_value = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    return precision_value\n",
    "\n",
    "def recall(predicted_labels, true_labels):\n",
    "    \"\"\"\n",
    "    Recall is True Positives / All Positive Labels\n",
    "    \n",
    "    Args:\n",
    "        predicted_labels (np.ndarray[int, 1]): the integer labels from the predictions. Uni-dimensional\n",
    "        true_labels (np.ndarray[int, 1]): the integer labels from the gold standard. Uni-dimensional\n",
    "    \n",
    "    Returns:\n",
    "        recall_value (double)\n",
    "        \n",
    "    \"\"\"\n",
    "    recall_value = 0.\n",
    "    TP = np.sum((predicted_labels == 1) & (true_labels == 1))\n",
    "    FN = np.sum((predicted_labels == 0) & (true_labels == 1))\n",
    "    recall_value = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    return recall_value\n",
    "\n",
    "def f1_score(predicted_labels, true_labels):\n",
    "    \"\"\"\n",
    "    F1 score is the harmonic mean of precision and recall\n",
    "    \n",
    "    Args:\n",
    "        predicted_labels (np.ndarray[int, 1]): the integer labels from the predictions. Uni-dimensional\n",
    "        true_labels (np.ndarray[int, 1]): the integer labels from the gold standard. Uni-dimensional\n",
    "    \n",
    "    Returns:\n",
    "        f1_score_value (double)\n",
    "        \n",
    "    \"\"\"\n",
    "    f1_score_value = 0.\n",
    "    P = precision(predicted_labels, true_labels)\n",
    "    R = recall(predicted_labels, true_labels)\n",
    "    f1_score_value = 2 * P * R / (P + R) if (P + R) > 0 else 0\n",
    "    return f1_score_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "cm = confusion_matrix(yTrue, yPred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "print(\"Precision Score:\", precision)\n",
    "\n",
    "recall = recall_score(y_true, y_pred)\n",
    "print(\"Recall Score:\", recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
